{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHBP5xzmrM4U"
      },
      "source": [
        "# Neural Machine Translation (NMT) - Translating English sentences to Vietnam sentences\n",
        "\n",
        "Machine Translation refers to translating phrases across languages using deep learning and specifically with RNN ( Recurrent Neural Nets ). Most of these are complex systems that is they are a combined system of various algorithms. But, at its core, NMT uses sequence-to-sequence ( seq2seq ) RNN cells. Such models could be character level but word level models remain common.\n",
        "\n",
        "![NMT system](https://3.bp.blogspot.com/-3Pbj_dvt0Vo/V-qe-Nl6P5I/AAAAAAAABQc/z0_6WtVWtvARtMk0i9_AtLeyyGyV6AI4wCLcB/s1600/nmt-model-fast.gif)\n",
        "\n",
        "I insist to change the runtime to a GPU runtime so that training could be faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVPRC0bOtBas"
      },
      "source": [
        "## What are we going to do?\n",
        "We will basically create an encoder-decoder LSTM model using [Keras Functional API](https://www.tensorflow.org/alpha/guide/keras/functional) ( with [TensorFlow](https://www.tensorflow.org/) ). We will convert the English sentences to VietNam, but why VietNam\n",
        "\n",
        "\n",
        "*   Has special characters and much complex.\n",
        "\n",
        "\n",
        "Here's an example,\n",
        "\n",
        "Hello --> Xin chào\n",
        "\n",
        "So, let's get started.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_r70epHozOt"
      },
      "source": [
        "## Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq4aH4u1uq5V"
      },
      "source": [
        "### 1) Importing the libraries\n",
        "\n",
        "We will import TensorFlow and Keras. From Keras, we import various modules which help in building NN layers, preprocess data and construct LSTM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qK2TWV1nm48Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gNBpE-SFtqY",
        "outputId": "a43b8fdb-0ca2-47ad-d203-1337e736929e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path_to_save = \"/content/drive/MyDrive/Colab Notebooks/LSTM\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnIg8HdTGW4o"
      },
      "source": [
        "### 2) Reading the data\n",
        "\n",
        "\n",
        "Our dataset which contains more than 30K pairs of English-VietNam phrases. This amazing dataset is available at http://www.manythings.org/anki/ and it also other 50+ sets of bilingual sentences. We download the dataset for English-VietNam phrases, unzip it and read it using [Pandas](https://pandas.pydata.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27OzmS-MIymc",
        "outputId": "270c1ac9-9c10-4de8-f8c9-7dc4ed04b586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-22 06:27:33--  http://www.manythings.org/anki/vie-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.92.44, 172.67.186.54, 2606:4700:3033::ac43:ba36, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.92.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 320614 (313K) [application/zip]\n",
            "Saving to: ‘vie-eng.zip’\n",
            "\n",
            "vie-eng.zip         100%[===================>] 313.10K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-06-22 06:27:33 (9.82 MB/s) - ‘vie-eng.zip’ saved [320614/320614]\n",
            "\n",
            "Archive:  vie-eng.zip\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: _about.txt              \n",
            "replace vie.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: vie.txt                 \n"
          ]
        }
      ],
      "source": [
        "\n",
        "!wget http://www.manythings.org/anki/vie-eng.zip -O vie-eng.zip\n",
        "!unzip vie-eng.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wKCgjP61nxxO"
      },
      "outputs": [],
      "source": [
        "lines = pd.read_table( 'vie.txt' , names=[ 'eng' , 'vie' ] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JsCTBHaOn6qu"
      },
      "outputs": [],
      "source": [
        "lines.reset_index( level=0 , inplace=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3hFTEqE7n74G",
        "outputId": "aa2506d7-7a6d-4181-ad8c-15655efde30a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-896a116f-eadc-4df6-8434-fcc8b73d01a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>eng</th>\n",
              "      <th>vie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Chạy!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Help!</td>\n",
              "      <td>Giúp tôi với!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go on.</td>\n",
              "      <td>Tiếp tục đi.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>Chào bạn.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hurry!</td>\n",
              "      <td>Nhanh lên nào!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8045</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Sê-lê-na Gô-mét đã được lựa chọn...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8046</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Selena Gomez đã được lựa chọn để...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8047</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Selena Gomez đã trở thành Đại sứ...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8048</th>\n",
              "      <td>The people here are particular about what they...</td>\n",
              "      <td>Những người ở đây khá là khó tính về khẩu vị ă...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8049</th>\n",
              "      <td>No matter how much you try to convince people ...</td>\n",
              "      <td>Cho dù bạn có thuyết phục mọi người rằng sô-cô...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8050 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-896a116f-eadc-4df6-8434-fcc8b73d01a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-896a116f-eadc-4df6-8434-fcc8b73d01a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-896a116f-eadc-4df6-8434-fcc8b73d01a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                  index  \\\n",
              "0                                                  Run!   \n",
              "1                                                 Help!   \n",
              "2                                                Go on.   \n",
              "3                                                Hello!   \n",
              "4                                                Hurry!   \n",
              "...                                                 ...   \n",
              "8045  In 2009, Selena Gomez became the youngest pers...   \n",
              "8046  In 2009, Selena Gomez became the youngest pers...   \n",
              "8047  In 2009, Selena Gomez became the youngest pers...   \n",
              "8048  The people here are particular about what they...   \n",
              "8049  No matter how much you try to convince people ...   \n",
              "\n",
              "                                                    eng  \\\n",
              "0                                                 Chạy!   \n",
              "1                                         Giúp tôi với!   \n",
              "2                                          Tiếp tục đi.   \n",
              "3                                             Chào bạn.   \n",
              "4                                        Nhanh lên nào!   \n",
              "...                                                 ...   \n",
              "8045  Vào năm 2009, Sê-lê-na Gô-mét đã được lựa chọn...   \n",
              "8046  Vào năm 2009, Selena Gomez đã được lựa chọn để...   \n",
              "8047  Vào năm 2009, Selena Gomez đã trở thành Đại sứ...   \n",
              "8048  Những người ở đây khá là khó tính về khẩu vị ă...   \n",
              "8049  Cho dù bạn có thuyết phục mọi người rằng sô-cô...   \n",
              "\n",
              "                                                    vie  \n",
              "0     CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
              "1     CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
              "2     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "3     CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
              "4     CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "...                                                 ...  \n",
              "8045  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "8046  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "8047  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "8048  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "8049  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
              "\n",
              "[8050 rows x 3 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g_ads9smoIOo"
      },
      "outputs": [],
      "source": [
        "lines.rename( columns={ 'index' : 'eng' , 'eng' : 'vie' , 'vie' : 'c' } , inplace=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ukpTM4BpVqNc",
        "outputId": "e3668f05-0c0b-4d21-b158-0164c0bd756f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>vie</th>\n",
              "      <th>c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Chạy!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Help!</td>\n",
              "      <td>Giúp tôi với!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go on.</td>\n",
              "      <td>Tiếp tục đi.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>Chào bạn.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hurry!</td>\n",
              "      <td>Nhanh lên nào!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      eng             vie                                                  c\n",
              "0    Run!           Chạy!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "1   Help!   Giúp tôi với!  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
              "2  Go on.    Tiếp tục đi.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
              "3  Hello!       Chào bạn.  CC-BY 2.0 (France) Attribution: tatoeba.org #3...\n",
              "4  Hurry!  Nhanh lên nào!  CC-BY 2.0 (France) Attribution: tatoeba.org #1..."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dgIdfjIRLDN"
      },
      "source": [
        "### 3) Preparing input data for the Encoder ( `encoder_input_data` )\n",
        "The Encoder model will be fed input data which are preprocessed English sentences. The preprocessing is done as follows :\n",
        "\n",
        "\n",
        "1.   Tokenizing the English sentences from `eng_lines`.\n",
        "2.   Determining the maximum length of the English sentence that's `max_input_length`.\n",
        "3.   Padding the `tokenized_eng_lines` to the max_input_length.\n",
        "4.   Determining the vocabulary size ( `num_eng_tokens` ) for English words.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xk5qXb6kpPxW"
      },
      "outputs": [],
      "source": [
        "eng_lines = list()\n",
        "for line in lines.eng:\n",
        "    eng_lines.append( line ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "up3Yfut1pYol"
      },
      "outputs": [],
      "source": [
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( eng_lines ) \n",
        "tokenized_eng_lines = tokenizer.texts_to_sequences( eng_lines ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McI0G_HnphMt",
        "outputId": "1598aa61-c302-447a-d442-2acc1707e03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English max length is 32\n"
          ]
        }
      ],
      "source": [
        "length_list = list()\n",
        "for token_seq in tokenized_eng_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_input_length = np.array( length_list ).max()\n",
        "print( 'English max length is {}'.format( max_input_length ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nazp0FX5plVF",
        "outputId": "3767f4d3-f1df-4f26-e29c-2293bc3c86e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder input data shape -> (8050, 32)\n"
          ]
        }
      ],
      "source": [
        "padded_eng_lines = preprocessing.sequence.pad_sequences( tokenized_eng_lines , maxlen=max_input_length , padding='post' )\n",
        "encoder_input_data = np.array( padded_eng_lines )\n",
        "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   9, 1350,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "      dtype=int32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_input_data[18]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2_ux1rZnDyY",
        "outputId": "9b5d0722-3c0d-4bd3-f83d-45a18a63ecf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of English tokens = 3802\n"
          ]
        }
      ],
      "source": [
        "eng_word_dict = tokenizer.word_index\n",
        "num_eng_tokens = len( eng_word_dict )+1\n",
        "print( 'Number of English tokens = {}'.format( num_eng_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'i': 1,\n",
              " 'to': 2,\n",
              " 'the': 3,\n",
              " 'tom': 4,\n",
              " 'you': 5,\n",
              " 'a': 6,\n",
              " 'is': 7,\n",
              " 'that': 8,\n",
              " 'he': 9,\n",
              " 'do': 10,\n",
              " 'in': 11,\n",
              " 'of': 12,\n",
              " 'me': 13,\n",
              " 'this': 14,\n",
              " 'have': 15,\n",
              " \"don't\": 16,\n",
              " 'it': 17,\n",
              " 'was': 18,\n",
              " 'my': 19,\n",
              " 'for': 20,\n",
              " \"i'm\": 21,\n",
              " 'are': 22,\n",
              " 'mary': 23,\n",
              " 'your': 24,\n",
              " 'we': 25,\n",
              " 'she': 26,\n",
              " 'what': 27,\n",
              " 'want': 28,\n",
              " 'be': 29,\n",
              " 'at': 30,\n",
              " 'with': 31,\n",
              " 'like': 32,\n",
              " 'on': 33,\n",
              " 'his': 34,\n",
              " 'think': 35,\n",
              " 'know': 36,\n",
              " 'not': 37,\n",
              " 'and': 38,\n",
              " 'can': 39,\n",
              " 'has': 40,\n",
              " 'did': 41,\n",
              " 'go': 42,\n",
              " 'very': 43,\n",
              " 'will': 44,\n",
              " 'how': 45,\n",
              " 'there': 46,\n",
              " \"didn't\": 47,\n",
              " 'going': 48,\n",
              " 'here': 49,\n",
              " 'time': 50,\n",
              " 'get': 51,\n",
              " \"it's\": 52,\n",
              " 'all': 53,\n",
              " 'up': 54,\n",
              " 'no': 55,\n",
              " \"can't\": 56,\n",
              " 'an': 57,\n",
              " 'as': 58,\n",
              " 'had': 59,\n",
              " 'about': 60,\n",
              " 'him': 61,\n",
              " 'one': 62,\n",
              " 'from': 63,\n",
              " 'why': 64,\n",
              " 'if': 65,\n",
              " 'when': 66,\n",
              " 'they': 67,\n",
              " 'but': 68,\n",
              " 'out': 69,\n",
              " 'more': 70,\n",
              " 'her': 71,\n",
              " 'said': 72,\n",
              " 'who': 73,\n",
              " 'by': 74,\n",
              " \"i'll\": 75,\n",
              " 'come': 76,\n",
              " 'need': 77,\n",
              " 'than': 78,\n",
              " 'would': 79,\n",
              " 'never': 80,\n",
              " \"isn't\": 81,\n",
              " 'home': 82,\n",
              " 'really': 83,\n",
              " \"i've\": 84,\n",
              " 'should': 85,\n",
              " \"doesn't\": 86,\n",
              " 'boston': 87,\n",
              " 'please': 88,\n",
              " \"you're\": 89,\n",
              " 'good': 90,\n",
              " 'could': 91,\n",
              " 'too': 92,\n",
              " 'much': 93,\n",
              " 'work': 94,\n",
              " 'school': 95,\n",
              " 'help': 96,\n",
              " 'french': 97,\n",
              " 'told': 98,\n",
              " 'been': 99,\n",
              " 'so': 100,\n",
              " 'some': 101,\n",
              " 'speak': 102,\n",
              " 'got': 103,\n",
              " 'where': 104,\n",
              " \"won't\": 105,\n",
              " 'take': 106,\n",
              " 'money': 107,\n",
              " 'tell': 108,\n",
              " 'now': 109,\n",
              " 'see': 110,\n",
              " 'just': 111,\n",
              " 'were': 112,\n",
              " 'last': 113,\n",
              " 'give': 114,\n",
              " 'today': 115,\n",
              " 'us': 116,\n",
              " 'still': 117,\n",
              " 'car': 118,\n",
              " 'anything': 119,\n",
              " 'tomorrow': 120,\n",
              " 'eat': 121,\n",
              " 'people': 122,\n",
              " 'three': 123,\n",
              " 'does': 124,\n",
              " \"tom's\": 125,\n",
              " 'thought': 126,\n",
              " 'book': 127,\n",
              " 'many': 128,\n",
              " 'years': 129,\n",
              " 'lot': 130,\n",
              " 'day': 131,\n",
              " \"he's\": 132,\n",
              " 'always': 133,\n",
              " 'sure': 134,\n",
              " 'new': 135,\n",
              " \"that's\": 136,\n",
              " 'left': 137,\n",
              " 'better': 138,\n",
              " 'only': 139,\n",
              " 'back': 140,\n",
              " 'let': 141,\n",
              " 'may': 142,\n",
              " 'made': 143,\n",
              " 'love': 144,\n",
              " 'house': 145,\n",
              " 'yesterday': 146,\n",
              " \"let's\": 147,\n",
              " 'father': 148,\n",
              " \"i'd\": 149,\n",
              " 'buy': 150,\n",
              " 'first': 151,\n",
              " \"what's\": 152,\n",
              " 'any': 153,\n",
              " 'understand': 154,\n",
              " 'after': 155,\n",
              " 'our': 156,\n",
              " 'before': 157,\n",
              " 'went': 158,\n",
              " 'mother': 159,\n",
              " 'next': 160,\n",
              " 'talk': 161,\n",
              " 'try': 162,\n",
              " 'over': 163,\n",
              " \"wasn't\": 164,\n",
              " 'way': 165,\n",
              " 'something': 166,\n",
              " 'ever': 167,\n",
              " 'knows': 168,\n",
              " 'night': 169,\n",
              " 'say': 170,\n",
              " 'wanted': 171,\n",
              " 'busy': 172,\n",
              " 'keep': 173,\n",
              " 'live': 174,\n",
              " \"we're\": 175,\n",
              " 'right': 176,\n",
              " 'doing': 177,\n",
              " 'must': 178,\n",
              " 'came': 179,\n",
              " 'name': 180,\n",
              " 'problem': 181,\n",
              " 'both': 182,\n",
              " 'old': 183,\n",
              " 'nothing': 184,\n",
              " 'leave': 185,\n",
              " 'play': 186,\n",
              " 'every': 187,\n",
              " \"couldn't\": 188,\n",
              " 'long': 189,\n",
              " 'am': 190,\n",
              " 'sorry': 191,\n",
              " 'little': 192,\n",
              " 'call': 193,\n",
              " 'everyone': 194,\n",
              " 'two': 195,\n",
              " 'make': 196,\n",
              " 'wait': 197,\n",
              " 'well': 198,\n",
              " 'afraid': 199,\n",
              " 'rain': 200,\n",
              " 'used': 201,\n",
              " 'food': 202,\n",
              " 'wants': 203,\n",
              " 'everything': 204,\n",
              " 'room': 205,\n",
              " 'bought': 206,\n",
              " \"there's\": 207,\n",
              " 'them': 208,\n",
              " 'english': 209,\n",
              " 'person': 210,\n",
              " 'week': 211,\n",
              " 'morning': 212,\n",
              " 'stay': 213,\n",
              " 'idea': 214,\n",
              " 'off': 215,\n",
              " 'able': 216,\n",
              " 'dog': 217,\n",
              " 'door': 218,\n",
              " 'year': 219,\n",
              " 'late': 220,\n",
              " 'look': 221,\n",
              " 'around': 222,\n",
              " 'again': 223,\n",
              " 'anyone': 224,\n",
              " 'or': 225,\n",
              " 'teacher': 226,\n",
              " 'other': 227,\n",
              " 'lives': 228,\n",
              " 'happen': 229,\n",
              " 'believe': 230,\n",
              " 'sleep': 231,\n",
              " 'stop': 232,\n",
              " 'down': 233,\n",
              " 'brother': 234,\n",
              " 'asked': 235,\n",
              " 'without': 236,\n",
              " 'find': 237,\n",
              " 'feel': 238,\n",
              " 'says': 239,\n",
              " 'himself': 240,\n",
              " 'away': 241,\n",
              " \"should've\": 242,\n",
              " 'into': 243,\n",
              " 'alone': 244,\n",
              " 'life': 245,\n",
              " 'another': 246,\n",
              " 'these': 247,\n",
              " 'young': 248,\n",
              " 'australia': 249,\n",
              " 'heard': 250,\n",
              " 'might': 251,\n",
              " 'open': 252,\n",
              " 'wish': 253,\n",
              " 'parents': 254,\n",
              " 'happened': 255,\n",
              " 'because': 256,\n",
              " 'read': 257,\n",
              " 'hear': 258,\n",
              " 'looking': 259,\n",
              " 'ask': 260,\n",
              " 'man': 261,\n",
              " 'playing': 262,\n",
              " 'coffee': 263,\n",
              " 'few': 264,\n",
              " 'took': 265,\n",
              " 'japan': 266,\n",
              " 'umbrella': 267,\n",
              " 'saw': 268,\n",
              " 'hard': 269,\n",
              " 'big': 270,\n",
              " 'even': 271,\n",
              " 'soon': 272,\n",
              " 'meet': 273,\n",
              " 'coming': 274,\n",
              " 'ago': 275,\n",
              " 'hour': 276,\n",
              " 'anymore': 277,\n",
              " 'study': 278,\n",
              " 'bad': 279,\n",
              " 'their': 280,\n",
              " 'met': 281,\n",
              " 'wrong': 282,\n",
              " 'water': 283,\n",
              " 'gave': 284,\n",
              " 'hope': 285,\n",
              " 'things': 286,\n",
              " 'already': 287,\n",
              " 'probably': 288,\n",
              " 'friend': 289,\n",
              " 'boy': 290,\n",
              " 'lost': 291,\n",
              " 'swim': 292,\n",
              " 'eating': 293,\n",
              " 'which': 294,\n",
              " 'phone': 295,\n",
              " 'use': 296,\n",
              " 'knew': 297,\n",
              " 'enough': 298,\n",
              " 'world': 299,\n",
              " \"mary's\": 300,\n",
              " \"you've\": 301,\n",
              " 'dinner': 302,\n",
              " 'students': 303,\n",
              " 'cold': 304,\n",
              " 'done': 305,\n",
              " 'everybody': 306,\n",
              " 'likes': 307,\n",
              " 'bus': 308,\n",
              " 'best': 309,\n",
              " 'waiting': 310,\n",
              " 'tonight': 311,\n",
              " 'most': 312,\n",
              " 'important': 313,\n",
              " 'lying': 314,\n",
              " 'children': 315,\n",
              " 'job': 316,\n",
              " 'finish': 317,\n",
              " 'died': 318,\n",
              " 'party': 319,\n",
              " 'tired': 320,\n",
              " 'cup': 321,\n",
              " 'each': 322,\n",
              " 'often': 323,\n",
              " 'pretty': 324,\n",
              " 'bed': 325,\n",
              " 'tv': 326,\n",
              " 'free': 327,\n",
              " 'true': 328,\n",
              " 'put': 329,\n",
              " \"she's\": 330,\n",
              " 'happy': 331,\n",
              " \"we'll\": 332,\n",
              " \"it'll\": 333,\n",
              " \"you'll\": 334,\n",
              " 'piano': 335,\n",
              " 'ate': 336,\n",
              " 'together': 337,\n",
              " 'early': 338,\n",
              " 'same': 339,\n",
              " 'times': 340,\n",
              " 'remember': 341,\n",
              " 'nobody': 342,\n",
              " 'finished': 343,\n",
              " 'ten': 344,\n",
              " 'computer': 345,\n",
              " 'once': 346,\n",
              " 'drink': 347,\n",
              " 'friends': 348,\n",
              " 'started': 349,\n",
              " 'train': 350,\n",
              " 'drive': 351,\n",
              " 'eyes': 352,\n",
              " 'watch': 353,\n",
              " 'change': 354,\n",
              " 'station': 355,\n",
              " 'usually': 356,\n",
              " 'while': 357,\n",
              " 'easy': 358,\n",
              " 'walk': 359,\n",
              " 'getting': 360,\n",
              " 'those': 361,\n",
              " 'books': 362,\n",
              " 'seems': 363,\n",
              " 'bicycle': 364,\n",
              " 'yet': 365,\n",
              " 'ready': 366,\n",
              " 'studying': 367,\n",
              " 'wife': 368,\n",
              " 'seem': 369,\n",
              " 'else': 370,\n",
              " 'learn': 371,\n",
              " 'since': 372,\n",
              " 'beautiful': 373,\n",
              " 'truth': 374,\n",
              " 'turned': 375,\n",
              " 'tree': 376,\n",
              " 'plan': 377,\n",
              " 'winter': 378,\n",
              " \"aren't\": 379,\n",
              " 'married': 380,\n",
              " 'apple': 381,\n",
              " 'reading': 382,\n",
              " 'looks': 383,\n",
              " 'changed': 384,\n",
              " 'number': 385,\n",
              " 'looked': 386,\n",
              " 'far': 387,\n",
              " 'called': 388,\n",
              " 'store': 389,\n",
              " 'matter': 390,\n",
              " 'anybody': 391,\n",
              " 'working': 392,\n",
              " 'win': 393,\n",
              " 'flowers': 394,\n",
              " 'white': 395,\n",
              " 'myself': 396,\n",
              " 'felt': 397,\n",
              " 'light': 398,\n",
              " 'homework': 399,\n",
              " 'sick': 400,\n",
              " 'japanese': 401,\n",
              " 'learning': 402,\n",
              " 'borrow': 403,\n",
              " 'trying': 404,\n",
              " 'decided': 405,\n",
              " 'thinks': 406,\n",
              " \"you'd\": 407,\n",
              " 'month': 408,\n",
              " 'office': 409,\n",
              " 'spend': 410,\n",
              " \"who's\": 411,\n",
              " 'miss': 412,\n",
              " 'lose': 413,\n",
              " 'trust': 414,\n",
              " 'show': 415,\n",
              " 'gets': 416,\n",
              " 'almost': 417,\n",
              " 'hot': 418,\n",
              " 'turn': 419,\n",
              " 'girl': 420,\n",
              " \"shouldn't\": 421,\n",
              " 'sister': 422,\n",
              " 'angry': 423,\n",
              " 'crying': 424,\n",
              " 'seen': 425,\n",
              " 'possible': 426,\n",
              " 'talking': 427,\n",
              " 'wonder': 428,\n",
              " 'exactly': 429,\n",
              " 'sound': 430,\n",
              " 'high': 431,\n",
              " 'monday': 432,\n",
              " 'famous': 433,\n",
              " '30': 434,\n",
              " 'hungry': 435,\n",
              " 'age': 436,\n",
              " 'needs': 437,\n",
              " 'under': 438,\n",
              " 'arrived': 439,\n",
              " 'war': 440,\n",
              " 'library': 441,\n",
              " 'someone': 442,\n",
              " 'fire': 443,\n",
              " 'table': 444,\n",
              " 'tried': 445,\n",
              " 'mind': 446,\n",
              " 'story': 447,\n",
              " 'younger': 448,\n",
              " 'city': 449,\n",
              " 'until': 450,\n",
              " 'evening': 451,\n",
              " 'hurt': 452,\n",
              " 'works': 453,\n",
              " 'waited': 454,\n",
              " 'park': 455,\n",
              " 'mean': 456,\n",
              " 'expensive': 457,\n",
              " 'makes': 458,\n",
              " 'being': 459,\n",
              " 'cat': 460,\n",
              " 'choice': 461,\n",
              " 'answer': 462,\n",
              " 'thinking': 463,\n",
              " \"we've\": 464,\n",
              " 'afternoon': 465,\n",
              " 'rains': 466,\n",
              " 'tea': 467,\n",
              " 'john': 468,\n",
              " 'small': 469,\n",
              " 'fallen': 470,\n",
              " 'difficult': 471,\n",
              " 'united': 472,\n",
              " 'hate': 473,\n",
              " 'sit': 474,\n",
              " 'music': 475,\n",
              " \"they're\": 476,\n",
              " 'rest': 477,\n",
              " 'camera': 478,\n",
              " 'watching': 479,\n",
              " 'words': 480,\n",
              " 'six': 481,\n",
              " 'beer': 482,\n",
              " 'five': 483,\n",
              " 'forget': 484,\n",
              " 'herself': 485,\n",
              " 'eaten': 486,\n",
              " 'actually': 487,\n",
              " 'breakfast': 488,\n",
              " 'picture': 489,\n",
              " 'surprised': 490,\n",
              " 'own': 491,\n",
              " 'lived': 492,\n",
              " 'weeks': 493,\n",
              " 'solve': 494,\n",
              " 'liked': 495,\n",
              " 'woman': 496,\n",
              " 'tennis': 497,\n",
              " '2': 498,\n",
              " 'quite': 499,\n",
              " 'stupid': 500,\n",
              " 'fast': 501,\n",
              " 'place': 502,\n",
              " 'write': 503,\n",
              " 'hair': 504,\n",
              " 'comes': 505,\n",
              " 'such': 506,\n",
              " 'killed': 507,\n",
              " 'swimming': 508,\n",
              " 'police': 509,\n",
              " 'cost': 510,\n",
              " 'gone': 511,\n",
              " 'move': 512,\n",
              " 'crazy': 513,\n",
              " 'son': 514,\n",
              " 'lunch': 515,\n",
              " 'rich': 516,\n",
              " 'pictures': 517,\n",
              " 'child': 518,\n",
              " 'hand': 519,\n",
              " 'meeting': 520,\n",
              " 'spent': 521,\n",
              " 'days': 522,\n",
              " 'near': 523,\n",
              " 'end': 524,\n",
              " '10': 525,\n",
              " 'girlfriend': 526,\n",
              " 'accident': 527,\n",
              " 'hurry': 528,\n",
              " 'guess': 529,\n",
              " 'kind': 530,\n",
              " 'mine': 531,\n",
              " 'pay': 532,\n",
              " 'touch': 533,\n",
              " 'yourself': 534,\n",
              " 'also': 535,\n",
              " 'cats': 536,\n",
              " \"where's\": 537,\n",
              " 'outside': 538,\n",
              " 'dogs': 539,\n",
              " 'speaks': 540,\n",
              " \"haven't\": 541,\n",
              " 'thank': 542,\n",
              " 'great': 543,\n",
              " 'explain': 544,\n",
              " 'letter': 545,\n",
              " 'class': 546,\n",
              " 'later': 547,\n",
              " 'catch': 548,\n",
              " 'promised': 549,\n",
              " 'thing': 550,\n",
              " 'earlier': 551,\n",
              " 'found': 552,\n",
              " 'certain': 553,\n",
              " 'born': 554,\n",
              " 'family': 555,\n",
              " 'through': 556,\n",
              " 'older': 557,\n",
              " 'save': 558,\n",
              " 'fired': 559,\n",
              " 'building': 560,\n",
              " 'wear': 561,\n",
              " 'kiss': 562,\n",
              " 'wine': 563,\n",
              " 'safe': 564,\n",
              " 'trip': 565,\n",
              " 'behind': 566,\n",
              " 'ride': 567,\n",
              " 'set': 568,\n",
              " 'mistake': 569,\n",
              " 'guitar': 570,\n",
              " 'closed': 571,\n",
              " 'became': 572,\n",
              " 'sat': 573,\n",
              " 'stopped': 574,\n",
              " 'large': 575,\n",
              " 'golf': 576,\n",
              " 'dress': 577,\n",
              " 'hospital': 578,\n",
              " 'restaurant': 579,\n",
              " \"hasn't\": 580,\n",
              " 'television': 581,\n",
              " 'abroad': 582,\n",
              " 'seemed': 583,\n",
              " 'impossible': 584,\n",
              " \"wouldn't\": 585,\n",
              " 'apologize': 586,\n",
              " 'hardly': 587,\n",
              " 'town': 588,\n",
              " 'living': 589,\n",
              " 'trouble': 590,\n",
              " 'leftovers': 591,\n",
              " 'questions': 592,\n",
              " 'heart': 593,\n",
              " 'sitting': 594,\n",
              " 'thirteen': 595,\n",
              " 'realize': 596,\n",
              " 'begin': 597,\n",
              " 'curious': 598,\n",
              " 'raining': 599,\n",
              " 'kept': 600,\n",
              " 'history': 601,\n",
              " 'knife': 602,\n",
              " 'shoes': 603,\n",
              " 'storm': 604,\n",
              " 'bottle': 605,\n",
              " 'stayed': 606,\n",
              " 'whatever': 607,\n",
              " 'dead': 608,\n",
              " 'milk': 609,\n",
              " 'student': 610,\n",
              " 'tokyo': 611,\n",
              " 'favorite': 612,\n",
              " 'wash': 613,\n",
              " 'wearing': 614,\n",
              " 'question': 615,\n",
              " 'paper': 616,\n",
              " 'fall': 617,\n",
              " 'daughter': 618,\n",
              " 'face': 619,\n",
              " 'baby': 620,\n",
              " 'having': 621,\n",
              " 'learned': 622,\n",
              " 'interesting': 623,\n",
              " 'known': 624,\n",
              " 'yen': 625,\n",
              " 'hours': 626,\n",
              " \"weren't\": 627,\n",
              " 'village': 628,\n",
              " 'hundred': 629,\n",
              " 'language': 630,\n",
              " 'owner': 631,\n",
              " 'reason': 632,\n",
              " 'whenever': 633,\n",
              " 'minutes': 634,\n",
              " 'post': 635,\n",
              " 'realized': 636,\n",
              " 'then': 637,\n",
              " 'become': 638,\n",
              " 'grandfather': 639,\n",
              " 'states': 640,\n",
              " 'planning': 641,\n",
              " 'friendship': 642,\n",
              " 'between': 643,\n",
              " 'along': 644,\n",
              " 'consider': 645,\n",
              " 'run': 646,\n",
              " 'agree': 647,\n",
              " 'ok': 648,\n",
              " 'shy': 649,\n",
              " 'faster': 650,\n",
              " 'worse': 651,\n",
              " 'won': 652,\n",
              " 'lied': 653,\n",
              " 'lie': 654,\n",
              " 'fun': 655,\n",
              " 'news': 656,\n",
              " 'christmas': 657,\n",
              " 'pain': 658,\n",
              " 'missing': 659,\n",
              " 'travel': 660,\n",
              " 'worried': 661,\n",
              " 'hotel': 662,\n",
              " 'arrested': 663,\n",
              " 'part': 664,\n",
              " 'song': 665,\n",
              " 'yours': 666,\n",
              " 'word': 667,\n",
              " 'advice': 668,\n",
              " 'thirty': 669,\n",
              " 'death': 670,\n",
              " 'women': 671,\n",
              " 'opened': 672,\n",
              " 'glad': 673,\n",
              " 'finally': 674,\n",
              " 'whole': 675,\n",
              " 'real': 676,\n",
              " 'mouth': 677,\n",
              " 'foreign': 678,\n",
              " 'ideas': 679,\n",
              " 'careful': 680,\n",
              " 'eleven': 681,\n",
              " 'nice': 682,\n",
              " 'telephone': 683,\n",
              " 'meat': 684,\n",
              " 'weather': 685,\n",
              " 'paid': 686,\n",
              " 'sleeping': 687,\n",
              " 'start': 688,\n",
              " 'quickly': 689,\n",
              " 'pick': 690,\n",
              " 'sugar': 691,\n",
              " 'dollars': 692,\n",
              " 'somebody': 693,\n",
              " 'half': 694,\n",
              " 'phoned': 695,\n",
              " 'river': 696,\n",
              " 'goes': 697,\n",
              " 'needed': 698,\n",
              " 'paris': 699,\n",
              " 'stars': 700,\n",
              " 'taking': 701,\n",
              " 'country': 702,\n",
              " 'telling': 703,\n",
              " '20': 704,\n",
              " 'airport': 705,\n",
              " 'boyfriend': 706,\n",
              " '000': 707,\n",
              " 'promise': 708,\n",
              " 'movie': 709,\n",
              " 'kid': 710,\n",
              " 'supposed': 711,\n",
              " 'doubt': 712,\n",
              " 'poem': 713,\n",
              " 'harder': 714,\n",
              " 'boys': 715,\n",
              " 'whether': 716,\n",
              " 'selling': 717,\n",
              " 'forgot': 718,\n",
              " 'nervous': 719,\n",
              " 'dream': 720,\n",
              " 'poor': 721,\n",
              " 'hide': 722,\n",
              " 'running': 723,\n",
              " 'injured': 724,\n",
              " 'walked': 725,\n",
              " 'broke': 726,\n",
              " \"how's\": 727,\n",
              " 'doctor': 728,\n",
              " 'cry': 729,\n",
              " 'ambulance': 730,\n",
              " 'agreed': 731,\n",
              " 'stand': 732,\n",
              " 'dirty': 733,\n",
              " 'thanks': 734,\n",
              " 'leg': 735,\n",
              " 'cheap': 736,\n",
              " 'frightened': 737,\n",
              " 'bag': 738,\n",
              " 'america': 739,\n",
              " 'window': 740,\n",
              " 'player': 741,\n",
              " 'hobby': 742,\n",
              " 'seeing': 743,\n",
              " 'gate': 744,\n",
              " 'caught': 745,\n",
              " 'street': 746,\n",
              " 'return': 747,\n",
              " 'making': 748,\n",
              " 'point': 749,\n",
              " 'chance': 750,\n",
              " 'others': 751,\n",
              " 'care': 752,\n",
              " 'close': 753,\n",
              " 'bit': 754,\n",
              " 'asking': 755,\n",
              " 'president': 756,\n",
              " 'wind': 757,\n",
              " 'accused': 758,\n",
              " 'movies': 759,\n",
              " 'government': 760,\n",
              " 'team': 761,\n",
              " 'maybe': 762,\n",
              " 'smoking': 763,\n",
              " 'quiet': 764,\n",
              " 'soup': 765,\n",
              " 'hopes': 766,\n",
              " 'air': 767,\n",
              " 'bring': 768,\n",
              " 'case': 769,\n",
              " 'began': 770,\n",
              " 'club': 771,\n",
              " 'shop': 772,\n",
              " 'novel': 773,\n",
              " 'plans': 774,\n",
              " 'cannot': 775,\n",
              " 'standing': 776,\n",
              " 'figure': 777,\n",
              " 'interested': 778,\n",
              " 'several': 779,\n",
              " 'saying': 780,\n",
              " 'choose': 781,\n",
              " 'birthday': 782,\n",
              " 'though': 783,\n",
              " 'saving': 784,\n",
              " 'sea': 785,\n",
              " 'convince': 786,\n",
              " 'visited': 787,\n",
              " 'inherited': 788,\n",
              " 'ages': 789,\n",
              " 'slowly': 790,\n",
              " 'built': 791,\n",
              " 'enjoy': 792,\n",
              " 'drunk': 793,\n",
              " 'enter': 794,\n",
              " 'joking': 795,\n",
              " 'divorced': 796,\n",
              " 'men': 797,\n",
              " 'shopping': 798,\n",
              " 'list': 799,\n",
              " 'laugh': 800,\n",
              " 'carefully': 801,\n",
              " 'hates': 802,\n",
              " 'tall': 803,\n",
              " 'fruit': 804,\n",
              " 'sing': 805,\n",
              " 'whose': 806,\n",
              " 'regret': 807,\n",
              " 'past': 808,\n",
              " 'fishing': 809,\n",
              " 'hat': 810,\n",
              " 'dropped': 811,\n",
              " 'helped': 812,\n",
              " 'cake': 813,\n",
              " 'lies': 814,\n",
              " \"we'd\": 815,\n",
              " 'lawyer': 816,\n",
              " 'shut': 817,\n",
              " 'heavy': 818,\n",
              " 'sweater': 819,\n",
              " 'scared': 820,\n",
              " 'riding': 821,\n",
              " 'strange': 822,\n",
              " 'calm': 823,\n",
              " 'business': 824,\n",
              " 'secret': 825,\n",
              " 'its': 826,\n",
              " 'ring': 827,\n",
              " 'girls': 828,\n",
              " 'complain': 829,\n",
              " 'least': 830,\n",
              " 'keys': 831,\n",
              " 'terrible': 832,\n",
              " 'husband': 833,\n",
              " 'weight': 834,\n",
              " 'takes': 835,\n",
              " 'carry': 836,\n",
              " 'pencil': 837,\n",
              " 'disappointed': 838,\n",
              " 'loss': 839,\n",
              " 'blood': 840,\n",
              " 'die': 841,\n",
              " 'discuss': 842,\n",
              " 'moved': 843,\n",
              " 'wake': 844,\n",
              " 'perhaps': 845,\n",
              " 'taught': 846,\n",
              " 'listened': 847,\n",
              " 'head': 848,\n",
              " 'break': 849,\n",
              " 'worry': 850,\n",
              " 'fish': 851,\n",
              " 'visit': 852,\n",
              " 'progress': 853,\n",
              " 'sell': 854,\n",
              " 'forgotten': 855,\n",
              " 'floor': 856,\n",
              " 'canada': 857,\n",
              " 'snow': 858,\n",
              " 'against': 859,\n",
              " 'concert': 860,\n",
              " 'summer': 861,\n",
              " 'during': 862,\n",
              " 'order': 863,\n",
              " 'red': 864,\n",
              " 'ones': 865,\n",
              " 'traffic': 866,\n",
              " 'lights': 867,\n",
              " 'suggestion': 868,\n",
              " 'newspaper': 869,\n",
              " 'spends': 870,\n",
              " 'cave': 871,\n",
              " 'laughed': 872,\n",
              " 'kidding': 873,\n",
              " 'fight': 874,\n",
              " 'bread': 875,\n",
              " 'innocent': 876,\n",
              " 'loves': 877,\n",
              " 'kill': 878,\n",
              " 'owe': 879,\n",
              " 'rather': 880,\n",
              " 'plays': 881,\n",
              " 'ugly': 882,\n",
              " 'inside': 883,\n",
              " 'adopted': 884,\n",
              " 'serious': 885,\n",
              " '6': 886,\n",
              " 'wrote': 887,\n",
              " 'wet': 888,\n",
              " 'failed': 889,\n",
              " 'match': 890,\n",
              " 'horse': 891,\n",
              " 'green': 892,\n",
              " 'fell': 893,\n",
              " 'sunday': 894,\n",
              " 'sleepy': 895,\n",
              " 'cooking': 896,\n",
              " 'entered': 897,\n",
              " 'tells': 898,\n",
              " 'healthy': 899,\n",
              " 'lit': 900,\n",
              " 'different': 901,\n",
              " 'truck': 902,\n",
              " 'memory': 903,\n",
              " 'deep': 904,\n",
              " 'pool': 905,\n",
              " 'graduated': 906,\n",
              " 'extremely': 907,\n",
              " 'brought': 908,\n",
              " 'black': 909,\n",
              " 'taxi': 910,\n",
              " 'seat': 911,\n",
              " 'upset': 912,\n",
              " 'fool': 913,\n",
              " 'problems': 914,\n",
              " 'buying': 915,\n",
              " 'wood': 916,\n",
              " 'saturday': 917,\n",
              " 'department': 918,\n",
              " 'jokes': 919,\n",
              " 'exam': 920,\n",
              " 'explanation': 921,\n",
              " 'calling': 922,\n",
              " 'skin': 923,\n",
              " 'loudly': 924,\n",
              " 'classmates': 925,\n",
              " 'garden': 926,\n",
              " 'completely': 927,\n",
              " 'kitchen': 928,\n",
              " 'company': 929,\n",
              " 'arrive': 930,\n",
              " 'sports': 931,\n",
              " 'seriously': 932,\n",
              " 'expected': 933,\n",
              " 'climbed': 934,\n",
              " 'follow': 935,\n",
              " 'likely': 936,\n",
              " 'medicine': 937,\n",
              " 'comfortable': 938,\n",
              " 'vegetables': 939,\n",
              " 'clean': 940,\n",
              " 'allowed': 941,\n",
              " 'thief': 942,\n",
              " 'address': 943,\n",
              " 'speaking': 944,\n",
              " 'dangerous': 945,\n",
              " 'minute': 946,\n",
              " 'common': 947,\n",
              " 'special': 948,\n",
              " 'dad': 949,\n",
              " \"he'd\": 950,\n",
              " 'traveled': 951,\n",
              " 'feeling': 952,\n",
              " 'stock': 953,\n",
              " 'repair': 954,\n",
              " 'refrigerator': 955,\n",
              " 'named': 956,\n",
              " 'lake': 957,\n",
              " 'worst': 958,\n",
              " 'awoke': 959,\n",
              " 'dark': 960,\n",
              " 'top': 961,\n",
              " \"man's\": 962,\n",
              " 'translate': 963,\n",
              " 'convinced': 964,\n",
              " 'purchase': 965,\n",
              " 'prison': 966,\n",
              " 'chinese': 967,\n",
              " 'impression': 968,\n",
              " 'happening': 969,\n",
              " 'rid': 970,\n",
              " 'less': 971,\n",
              " 'plane': 972,\n",
              " 'ridden': 973,\n",
              " 'antique': 974,\n",
              " 'fat': 975,\n",
              " 'vote': 976,\n",
              " 'pizza': 977,\n",
              " 'law': 978,\n",
              " 'moving': 979,\n",
              " 'key': 980,\n",
              " 'bike': 981,\n",
              " \"could've\": 982,\n",
              " 'slept': 983,\n",
              " 'hold': 984,\n",
              " 'side': 985,\n",
              " 'mad': 986,\n",
              " 'box': 987,\n",
              " 'visa': 988,\n",
              " 'advise': 989,\n",
              " 'gotten': 990,\n",
              " 'short': 991,\n",
              " 'singing': 992,\n",
              " 'soccer': 993,\n",
              " 'korean': 994,\n",
              " 'writing': 995,\n",
              " 'example': 996,\n",
              " 'backwards': 997,\n",
              " 'roof': 998,\n",
              " 'gun': 999,\n",
              " 'drank': 1000,\n",
              " ...}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eng_word_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EAdyRwx2B98r"
      },
      "outputs": [],
      "source": [
        "fp = open(os.path.join(path_to_save,'eng_word_dict.pkl'), 'wb')\n",
        "pickle.dump(eng_word_dict, fp)\n",
        "fp.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRwAd310SPkG"
      },
      "source": [
        "### 4) Preparing input data for the Decoder ( `decoder_input_data` )\n",
        "The Decoder model will be fed the preprocessed VietNam lines. The preprocessing steps are similar to the ones which are above. This one step is carried out before the other steps.\n",
        "\n",
        "\n",
        "*   Append `<START>` tag at the first position in  each VietNam sentence.\n",
        "*   Append `<END>` tag at the last position in  each VietNam sentence.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0a022jLIprId"
      },
      "outputs": [],
      "source": [
        "vie_lines = list()\n",
        "for line in lines.vie:\n",
        "    vie_lines.append( '<START> ' + line + ' <END>' )  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iNeYte1fqEte"
      },
      "outputs": [],
      "source": [
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( vie_lines ) \n",
        "tokenized_vie_lines = tokenizer.texts_to_sequences( vie_lines ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GulPMF4vqNlF",
        "outputId": "939154eb-11f0-4926-eadd-0d3ce8dbd8da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vietnam max length is 43\n"
          ]
        }
      ],
      "source": [
        "length_list = list()\n",
        "for token_seq in tokenized_vie_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list ).max()\n",
        "print( 'Vietnam max length is {}'.format( max_output_length ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ft5igLnqTAe",
        "outputId": "d7dfda97-669c-4459-bc6f-d1d26116b106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder input data shape -> (8050, 43)\n"
          ]
        }
      ],
      "source": [
        "padded_vie_lines = preprocessing.sequence.pad_sequences( tokenized_vie_lines , maxlen=max_output_length, padding='post' )\n",
        "decoder_input_data = np.array( padded_vie_lines )\n",
        "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deB0oX_0pj8R",
        "outputId": "12d6eefe-5af7-487d-e2d3-0bb3e3cf9bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Vietnam tokens = 2384\n"
          ]
        }
      ],
      "source": [
        "vie_word_dict = tokenizer.word_index\n",
        "num_vie_tokens = len( vie_word_dict )+1\n",
        "print( 'Number of Vietnam tokens = {}'.format( num_vie_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HHSDqtg7CWm_"
      },
      "outputs": [],
      "source": [
        "fp = open(os.path.join(path_to_save,'vie_word_dict.pkl'), 'wb')\n",
        "pickle.dump(vie_word_dict, fp)\n",
        "fp.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJTcSlygTQ_V"
      },
      "source": [
        "### 5) Preparing target data for the Decoder ( decoder_target_data ) \n",
        "\n",
        "We take a copy of `tokenized_mar_lines` and modify it like this.\n",
        "\n",
        "\n",
        "\n",
        "1.   We remove the `<start>` tag which we appended earlier. Hence, the word ( which is `<start>` in this case  ) will be removed.\n",
        "2.   Convert the `padded_mar_lines` ( ones which do not have `<start>` tag ) to one-hot vectors.\n",
        "\n",
        "For example :\n",
        "\n",
        "```\n",
        " [ '<start>' , 'hello' , 'world' , '<end>' ]\n",
        "\n",
        "```\n",
        "\n",
        "wil become \n",
        "\n",
        "```\n",
        " [ 'hello' , 'world' , '<end>' ]\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPCTmeL7qj3T",
        "outputId": "dccb73d0-5273-463d-f874-031a22b8e5de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder target data shape -> (8050, 43, 2384)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "decoder_target_data = list()\n",
        "for token_seq in tokenized_vie_lines:\n",
        "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
        "    \n",
        "padded_vie_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
        "onehot_vie_lines = utils.to_categorical( padded_vie_lines , num_vie_tokens )\n",
        "decoder_target_data = np.array( onehot_vie_lines )\n",
        "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KS5gWlcpFT1"
      },
      "source": [
        "## Defining and Training the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_N71uykUPbe"
      },
      "source": [
        "### 1) Defining the Encoder-Decoder model\n",
        "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
        "\n",
        "\n",
        "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
        "*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n",
        "*   LSTM layer : Provide access to Long-Short Term cells.\n",
        "\n",
        "Working : \n",
        "\n",
        "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
        "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
        "3.   These states are set in the LSTM cell of the decoder.\n",
        "4.   The decoder_input_data comes in through the Embedding layer.\n",
        "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqb4Bps1s_Lr",
        "outputId": "0eea0e48-3dea-4752-e890-e230e7ff3062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " enc_input (InputLayer)         [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dec_input (InputLayer)         [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " enc_embedding (Embedding)      (None, None, 256)    973312      ['enc_input[0][0]']              \n",
            "                                                                                                  \n",
            " dec_embedding (Embedding)      (None, None, 256)    610304      ['dec_input[0][0]']              \n",
            "                                                                                                  \n",
            " enc_output (LSTM)              [(None, 128),        197120      ['enc_embedding[0][0]']          \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)            [(None, None, 128),  197120      ['dec_embedding[0][0]',          \n",
            "                                 (None, 128),                     'enc_output[0][1]',             \n",
            "                                 (None, 128)]                     'enc_output[0][2]']             \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)          (None, None, 2384)   307536      ['decoder_lstm[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,285,392\n",
            "Trainable params: 2,285,392\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=( None , ), name = 'enc_input')\n",
        "encoder_embedding = tf.keras.layers.Embedding( num_eng_tokens, 256 , mask_zero=True,name = 'enc_embedding') (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 128 , return_state=True,name = 'enc_output'  )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ), name = 'dec_input')\n",
        "decoder_embedding = tf.keras.layers.Embedding( num_vie_tokens, 256 , mask_zero=True,name = 'dec_embedding') (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 128 , return_state=True , return_sequences=True, name = 'decoder_lstm')\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( num_vie_tokens , activation=tf.keras.activations.softmax, name = 'decoder_dense' ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9g_8sR7WWf3"
      },
      "source": [
        "### 2) Training the model\n",
        "We train the model for a number of epochs with RMSprop optimizer and categorical crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnd2H27qt4Hy",
        "outputId": "ea66edcd-e54c-40e8-e696-552a357eb01f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33/33 - 41s - loss: 1.4244 - 41s/epoch - 1s/step\n",
            "Epoch 2/100\n",
            "33/33 - 34s - loss: 1.2562 - 34s/epoch - 1s/step\n",
            "Epoch 3/100\n",
            "33/33 - 33s - loss: 1.2040 - 33s/epoch - 1s/step\n",
            "Epoch 4/100\n",
            "33/33 - 33s - loss: 1.1637 - 33s/epoch - 1s/step\n",
            "Epoch 5/100\n",
            "33/33 - 35s - loss: 1.1324 - 35s/epoch - 1s/step\n",
            "Epoch 6/100\n",
            "33/33 - 35s - loss: 1.1048 - 35s/epoch - 1s/step\n",
            "Epoch 7/100\n",
            "33/33 - 33s - loss: 1.0782 - 33s/epoch - 1s/step\n",
            "Epoch 8/100\n",
            "33/33 - 33s - loss: 1.0505 - 33s/epoch - 1s/step\n",
            "Epoch 9/100\n",
            "33/33 - 33s - loss: 1.0227 - 33s/epoch - 1s/step\n",
            "Epoch 10/100\n",
            "33/33 - 33s - loss: 0.9967 - 33s/epoch - 1s/step\n",
            "Epoch 11/100\n",
            "33/33 - 33s - loss: 0.9729 - 33s/epoch - 1s/step\n",
            "Epoch 12/100\n",
            "33/33 - 33s - loss: 0.9502 - 33s/epoch - 1s/step\n",
            "Epoch 13/100\n",
            "33/33 - 36s - loss: 0.9301 - 36s/epoch - 1s/step\n",
            "Epoch 14/100\n",
            "33/33 - 33s - loss: 0.9108 - 33s/epoch - 1s/step\n",
            "Epoch 15/100\n",
            "33/33 - 35s - loss: 0.8926 - 35s/epoch - 1s/step\n",
            "Epoch 16/100\n",
            "33/33 - 33s - loss: 0.8758 - 33s/epoch - 1s/step\n",
            "Epoch 17/100\n",
            "33/33 - 33s - loss: 0.8584 - 33s/epoch - 1s/step\n",
            "Epoch 18/100\n",
            "33/33 - 33s - loss: 0.8424 - 33s/epoch - 1s/step\n",
            "Epoch 19/100\n",
            "33/33 - 33s - loss: 0.8256 - 33s/epoch - 1s/step\n",
            "Epoch 20/100\n",
            "33/33 - 33s - loss: 0.8096 - 33s/epoch - 1s/step\n",
            "Epoch 21/100\n",
            "33/33 - 33s - loss: 0.7945 - 33s/epoch - 1s/step\n",
            "Epoch 22/100\n",
            "33/33 - 33s - loss: 0.7791 - 33s/epoch - 1s/step\n",
            "Epoch 23/100\n",
            "33/33 - 33s - loss: 0.7643 - 33s/epoch - 1s/step\n",
            "Epoch 24/100\n",
            "33/33 - 35s - loss: 0.7489 - 35s/epoch - 1s/step\n",
            "Epoch 25/100\n",
            "33/33 - 33s - loss: 0.7353 - 33s/epoch - 1s/step\n",
            "Epoch 26/100\n",
            "33/33 - 33s - loss: 0.7204 - 33s/epoch - 1s/step\n",
            "Epoch 27/100\n",
            "33/33 - 33s - loss: 0.7060 - 33s/epoch - 1s/step\n",
            "Epoch 28/100\n",
            "33/33 - 33s - loss: 0.6925 - 33s/epoch - 1s/step\n",
            "Epoch 29/100\n",
            "33/33 - 33s - loss: 0.6787 - 33s/epoch - 1s/step\n",
            "Epoch 30/100\n",
            "33/33 - 33s - loss: 0.6653 - 33s/epoch - 1s/step\n",
            "Epoch 31/100\n",
            "33/33 - 33s - loss: 0.6523 - 33s/epoch - 1s/step\n",
            "Epoch 32/100\n",
            "33/33 - 33s - loss: 0.6390 - 33s/epoch - 1s/step\n",
            "Epoch 33/100\n",
            "33/33 - 35s - loss: 0.6260 - 35s/epoch - 1s/step\n",
            "Epoch 34/100\n",
            "33/33 - 33s - loss: 0.6141 - 33s/epoch - 1s/step\n",
            "Epoch 35/100\n",
            "33/33 - 33s - loss: 0.6016 - 33s/epoch - 1s/step\n",
            "Epoch 36/100\n",
            "33/33 - 33s - loss: 0.5893 - 33s/epoch - 1s/step\n",
            "Epoch 37/100\n",
            "33/33 - 33s - loss: 0.5771 - 33s/epoch - 1s/step\n",
            "Epoch 38/100\n",
            "33/33 - 33s - loss: 0.5653 - 33s/epoch - 1s/step\n",
            "Epoch 39/100\n",
            "33/33 - 33s - loss: 0.5538 - 33s/epoch - 1s/step\n",
            "Epoch 40/100\n",
            "33/33 - 33s - loss: 0.5423 - 33s/epoch - 1s/step\n",
            "Epoch 41/100\n",
            "33/33 - 33s - loss: 0.5312 - 33s/epoch - 1s/step\n",
            "Epoch 42/100\n",
            "33/33 - 33s - loss: 0.5196 - 33s/epoch - 1s/step\n",
            "Epoch 43/100\n",
            "33/33 - 35s - loss: 0.5093 - 35s/epoch - 1s/step\n",
            "Epoch 44/100\n",
            "33/33 - 33s - loss: 0.4985 - 33s/epoch - 1s/step\n",
            "Epoch 45/100\n",
            "33/33 - 33s - loss: 0.4880 - 33s/epoch - 1s/step\n",
            "Epoch 46/100\n",
            "33/33 - 33s - loss: 0.4774 - 33s/epoch - 1s/step\n",
            "Epoch 47/100\n",
            "33/33 - 33s - loss: 0.4680 - 33s/epoch - 1s/step\n",
            "Epoch 48/100\n",
            "33/33 - 33s - loss: 0.4578 - 33s/epoch - 1s/step\n",
            "Epoch 49/100\n",
            "33/33 - 33s - loss: 0.4481 - 33s/epoch - 1s/step\n",
            "Epoch 50/100\n",
            "33/33 - 33s - loss: 0.4386 - 33s/epoch - 1s/step\n",
            "Epoch 51/100\n",
            "33/33 - 33s - loss: 0.4287 - 33s/epoch - 1s/step\n",
            "Epoch 52/100\n",
            "33/33 - 35s - loss: 0.4198 - 35s/epoch - 1s/step\n",
            "Epoch 53/100\n",
            "33/33 - 33s - loss: 0.4108 - 33s/epoch - 1s/step\n",
            "Epoch 54/100\n",
            "33/33 - 33s - loss: 0.4020 - 33s/epoch - 1s/step\n",
            "Epoch 55/100\n",
            "33/33 - 34s - loss: 0.3933 - 34s/epoch - 1s/step\n",
            "Epoch 56/100\n",
            "33/33 - 34s - loss: 0.3845 - 34s/epoch - 1s/step\n",
            "Epoch 57/100\n",
            "33/33 - 34s - loss: 0.3767 - 34s/epoch - 1s/step\n",
            "Epoch 58/100\n",
            "33/33 - 34s - loss: 0.3683 - 34s/epoch - 1s/step\n",
            "Epoch 59/100\n",
            "33/33 - 34s - loss: 0.3598 - 34s/epoch - 1s/step\n",
            "Epoch 60/100\n",
            "33/33 - 33s - loss: 0.3529 - 33s/epoch - 1s/step\n",
            "Epoch 61/100\n",
            "33/33 - 35s - loss: 0.3450 - 35s/epoch - 1s/step\n",
            "Epoch 62/100\n",
            "33/33 - 34s - loss: 0.3370 - 34s/epoch - 1s/step\n",
            "Epoch 63/100\n",
            "33/33 - 33s - loss: 0.3298 - 33s/epoch - 1s/step\n",
            "Epoch 64/100\n",
            "33/33 - 33s - loss: 0.3224 - 33s/epoch - 1s/step\n",
            "Epoch 65/100\n",
            "33/33 - 33s - loss: 0.3157 - 33s/epoch - 1s/step\n",
            "Epoch 66/100\n",
            "33/33 - 33s - loss: 0.3083 - 33s/epoch - 1s/step\n",
            "Epoch 67/100\n",
            "33/33 - 33s - loss: 0.3015 - 33s/epoch - 1s/step\n",
            "Epoch 68/100\n",
            "33/33 - 33s - loss: 0.2952 - 33s/epoch - 1s/step\n",
            "Epoch 69/100\n",
            "33/33 - 33s - loss: 0.2890 - 33s/epoch - 1s/step\n",
            "Epoch 70/100\n",
            "33/33 - 33s - loss: 0.2815 - 33s/epoch - 1s/step\n",
            "Epoch 71/100\n",
            "33/33 - 35s - loss: 0.2761 - 35s/epoch - 1s/step\n",
            "Epoch 72/100\n",
            "33/33 - 33s - loss: 0.2703 - 33s/epoch - 1s/step\n",
            "Epoch 73/100\n",
            "33/33 - 33s - loss: 0.2634 - 33s/epoch - 1s/step\n",
            "Epoch 74/100\n",
            "33/33 - 33s - loss: 0.2584 - 33s/epoch - 1s/step\n",
            "Epoch 75/100\n",
            "33/33 - 33s - loss: 0.2516 - 33s/epoch - 1s/step\n",
            "Epoch 76/100\n",
            "33/33 - 33s - loss: 0.2464 - 33s/epoch - 1s/step\n",
            "Epoch 77/100\n",
            "33/33 - 36s - loss: 0.2406 - 36s/epoch - 1s/step\n",
            "Epoch 78/100\n",
            "33/33 - 35s - loss: 0.2354 - 35s/epoch - 1s/step\n",
            "Epoch 79/100\n",
            "33/33 - 33s - loss: 0.2299 - 33s/epoch - 1s/step\n",
            "Epoch 80/100\n",
            "33/33 - 35s - loss: 0.2245 - 35s/epoch - 1s/step\n",
            "Epoch 81/100\n",
            "33/33 - 34s - loss: 0.2192 - 34s/epoch - 1s/step\n",
            "Epoch 82/100\n",
            "33/33 - 33s - loss: 0.2148 - 33s/epoch - 1s/step\n",
            "Epoch 83/100\n",
            "33/33 - 34s - loss: 0.2088 - 34s/epoch - 1s/step\n",
            "Epoch 84/100\n",
            "33/33 - 34s - loss: 0.2052 - 34s/epoch - 1s/step\n",
            "Epoch 85/100\n",
            "33/33 - 34s - loss: 0.2000 - 34s/epoch - 1s/step\n",
            "Epoch 86/100\n",
            "33/33 - 34s - loss: 0.1956 - 34s/epoch - 1s/step\n",
            "Epoch 87/100\n",
            "33/33 - 34s - loss: 0.1912 - 34s/epoch - 1s/step\n",
            "Epoch 88/100\n",
            "33/33 - 34s - loss: 0.1865 - 34s/epoch - 1s/step\n",
            "Epoch 89/100\n",
            "33/33 - 34s - loss: 0.1822 - 34s/epoch - 1s/step\n",
            "Epoch 90/100\n",
            "33/33 - 35s - loss: 0.1779 - 35s/epoch - 1s/step\n",
            "Epoch 91/100\n",
            "33/33 - 34s - loss: 0.1747 - 34s/epoch - 1s/step\n",
            "Epoch 92/100\n",
            "33/33 - 33s - loss: 0.1703 - 33s/epoch - 1s/step\n",
            "Epoch 93/100\n",
            "33/33 - 34s - loss: 0.1663 - 34s/epoch - 1s/step\n",
            "Epoch 94/100\n",
            "33/33 - 34s - loss: 0.1622 - 34s/epoch - 1s/step\n",
            "Epoch 95/100\n",
            "33/33 - 34s - loss: 0.1592 - 34s/epoch - 1s/step\n",
            "Epoch 96/100\n",
            "33/33 - 34s - loss: 0.1545 - 34s/epoch - 1s/step\n",
            "Epoch 97/100\n",
            "33/33 - 34s - loss: 0.1518 - 34s/epoch - 1s/step\n",
            "Epoch 98/100\n",
            "33/33 - 34s - loss: 0.1480 - 34s/epoch - 1s/step\n",
            "Epoch 99/100\n",
            "33/33 - 35s - loss: 0.1442 - 35s/epoch - 1s/step\n",
            "Epoch 100/100\n",
            "33/33 - 34s - loss: 0.1410 - 34s/epoch - 1s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1595ed0fd0>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=200, epochs=10) \n",
        "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=100, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-d3SYw-dqR8",
        "outputId": "bd1a6d1b-cdab-4626-88bb-ceca39924988"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/LSTM/saved_model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/LSTM/saved_model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1599981510> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f15962adf50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "model.save(os.path.join(path_to_save,'saved_model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eeqv_vH5pMpb"
      },
      "source": [
        "## Inferencing on the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-dgKqBECnMj",
        "outputId": "b735b8cb-9a8f-42cb-d2e1-2957d54c4c8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-23 10:32:52.504607: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:52.705832: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:53.221574: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:53.504454: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:53.518852: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:54.171263: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:54.183400: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:54.354027: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:54.669536: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:54.682165: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:55.647991: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:55.674896: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:56.085981: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:56.100333: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:56.395543: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:56.406518: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:57.094881: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:57.168025: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:57.183174: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:57.285986: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:57.395203: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:57.406510: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:57.458963: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:57.469920: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:57.934396: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:58.044666: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:58.056183: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
            "2022-06-23 10:32:58.471266: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "#### thay path\n",
        "path = r\"LSTM\"\n",
        "# ####\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "eng_word_dict = dict()\n",
        "vie_word_dict = dict()\n",
        "fp = open(os.path.join(path,'eng_word_dict.pkl'), 'rb')\n",
        "eng_word_dict = pickle.load(fp)\n",
        "fp.close()\n",
        "fp = open(os.path.join(path,'vie_word_dict.pkl'), 'rb')\n",
        "vie_word_dict = pickle.load(fp)\n",
        "fp.close()\n",
        "num_eng_tokens = len(eng_word_dict) + 1\n",
        "num_vie_tokens = len(vie_word_dict) + 1\n",
        "\n",
        "model = tf.keras.models.load_model(os.path.join(path,'saved_model'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UNhVkiZLvdTq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_inference_models():\n",
        "    encoder_inputs = model.get_layer(\"enc_input\").output\n",
        "    encoder_outputs, state_h , state_c = model.get_layer(\"enc_output\").output\n",
        "    encoder_states = [ state_h , state_c ]\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 128 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 128 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_embedding = model.get_layer(\"dec_embedding\")\n",
        "    decoder_inputs = model.get_layer(\"dec_input\")\n",
        "\n",
        "    decoder_outputs, state_h, state_c = model.get_layer(\"decoder_lstm\")(\n",
        "        decoder_embedding.output , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = model.get_layer(\"decoder_dense\")(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs.output] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model\n",
        "\n",
        "max_input_length = 32\n",
        "max_output_length = 43\n",
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( eng_word_dict[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djEPrfJBmZE-"
      },
      "source": [
        "### 2) Making some translations\n",
        "\n",
        "\n",
        "1.   First, we take a English sequence and predict the state values using `enc_model`.\n",
        "2.   We set the state values in the decoder's LSTM.\n",
        "3.   Then, we generate a sequence which contains the `<start>` element.\n",
        "4.   We input this sequence in the `dec_model`.\n",
        "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
        "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum sequence length.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unknown error occured\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/sungjinwoo22/Documents/CourseCS/TinhToanDaPhuongTien/CS232.M21/Machine_translation.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sungjinwoo22/Documents/CourseCS/TinhToanDaPhuongTien/CS232.M21/Machine_translation.ipynb#ch0000028?line=23'>24</a>\u001b[0m r\u001b[39m.\u001b[39madjust_for_ambient_noise(source2, duration\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sungjinwoo22/Documents/CourseCS/TinhToanDaPhuongTien/CS232.M21/Machine_translation.ipynb#ch0000028?line=25'>26</a>\u001b[0m \u001b[39m#listens for the user's input\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sungjinwoo22/Documents/CourseCS/TinhToanDaPhuongTien/CS232.M21/Machine_translation.ipynb#ch0000028?line=26'>27</a>\u001b[0m audio2 \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mlisten(source2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sungjinwoo22/Documents/CourseCS/TinhToanDaPhuongTien/CS232.M21/Machine_translation.ipynb#ch0000028?line=28'>29</a>\u001b[0m \u001b[39m# Using google to recognize audio\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sungjinwoo22/Documents/CourseCS/TinhToanDaPhuongTien/CS232.M21/Machine_translation.ipynb#ch0000028?line=29'>30</a>\u001b[0m MyText \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mrecognize_google(audio2)\n",
            "File \u001b[0;32m~/Documents/CourseCS/TinhToanDaPhuongTien/CS232.M21/env/lib/python3.8/site-packages/speech_recognition/__init__.py:652\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[0;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m phrase_time_limit \u001b[39mand\u001b[39;00m elapsed_time \u001b[39m-\u001b[39m phrase_start_time \u001b[39m>\u001b[39m phrase_time_limit:\n\u001b[1;32m    650\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m buffer \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mread(source\u001b[39m.\u001b[39;49mCHUNK)\n\u001b[1;32m    653\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mbreak\u001b[39;00m  \u001b[39m# reached end of the stream\u001b[39;00m\n\u001b[1;32m    654\u001b[0m frames\u001b[39m.\u001b[39mappend(buffer)\n",
            "File \u001b[0;32m~/Documents/CourseCS/TinhToanDaPhuongTien/CS232.M21/env/lib/python3.8/site-packages/speech_recognition/__init__.py:161\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, size):\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpyaudio_stream\u001b[39m.\u001b[39;49mread(size, exception_on_overflow\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/Documents/CourseCS/TinhToanDaPhuongTien/CS232.M21/env/lib/python3.8/site-packages/pyaudio.py:608\u001b[0m, in \u001b[0;36mStream.read\u001b[0;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_input:\n\u001b[1;32m    605\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot input stream\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    606\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m--> 608\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39;49mread_stream(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream, num_frames, exception_on_overflow)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import playsound\n",
        "import os\n",
        "\n",
        "# Initialize the recognizer\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Loop infinitely for user to\n",
        "# speak\n",
        " \n",
        "while(1):   \n",
        "     \n",
        "    # Exception handling to handle\n",
        "    # exceptions at the runtime\n",
        "    try:\n",
        "         \n",
        "        # use the microphone as source for input.\n",
        "        with sr.Microphone() as source2:\n",
        "             \n",
        "            # wait for a second to let the recognizer\n",
        "            # adjust the energy threshold based on\n",
        "            # the surrounding noise level\n",
        "            r.adjust_for_ambient_noise(source2, duration=0.2)\n",
        "             \n",
        "            #listens for the user's input\n",
        "            audio2 = r.listen(source2)\n",
        "             \n",
        "            # Using google to recognize audio\n",
        "            MyText = r.recognize_google(audio2)\n",
        "            MyText = MyText.lower()\n",
        " \n",
        "            print(\"Did you say: \"+ MyText)\n",
        "\n",
        "            enc_model , dec_model = make_inference_models()\n",
        "            \n",
        "            states_values = enc_model.predict( str_to_tokens( MyText ) )\n",
        "            #states_values = enc_model.predict( encoder_input_data[ epoch ] )\n",
        "            empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "            empty_target_seq[0, 0] = vie_word_dict['start']\n",
        "            stop_condition = False\n",
        "            decoded_translation = ''\n",
        "            while not stop_condition :\n",
        "                dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "                sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "                sampled_word = None\n",
        "                for word , index in vie_word_dict.items() :\n",
        "                    if sampled_word_index == index :\n",
        "                        decoded_translation += ' {}'.format( word )\n",
        "                        sampled_word = word\n",
        "                \n",
        "                if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "                    stop_condition = True\n",
        "                    \n",
        "                empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "                empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "                states_values = [ h , c ] \n",
        "\n",
        "            print( decoded_translation[:-4] )\n",
        "                \n",
        "            \n",
        "            # text = \"Em nhà ở đâu thế\" \n",
        "            output = gTTS(decoded_translation[:-4], lang=\"vi\", slow=False)\n",
        "            output.save(\"output.mp3\")\n",
        "            playsound.playsound('output.mp3', True)\n",
        "            # os.remove(\"output.mp3\")\n",
        "             \n",
        "    except sr.RequestError as e:\n",
        "        print(\"Could not request results; {0}\".format(e))\n",
        "         \n",
        "    except sr.UnknownValueError:\n",
        "        print(\"unknown error occured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "d8HyTqKU2mI6",
        "outputId": "d8f9655d-3e5e-4f50-e718-e54b320f6259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            " chào bạn\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            " bạn là cái nào thế\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            " chúng tôi biết\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            " chúng tôi biết\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            " hãy nói thật đi\n"
          ]
        }
      ],
      "source": [
        "\n",
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "#for epoch in range( encoder_input_data.shape[0] ):\n",
        "while(1):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter eng sentence : ' ) ) )\n",
        "    #states_values = enc_model.predict( encoder_input_data[ epoch ] )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = vie_word_dict['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in vie_word_dict.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation[:-4] )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "a_r70epHozOt",
        "Yq4aH4u1uq5V",
        "6KS5gWlcpFT1",
        "M_N71uykUPbe"
      ],
      "name": "Machine translation.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "1ebb9218ba69dfec232a4bf5825cdced791a9637c6b00b87e186daf3b4a94a0a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
