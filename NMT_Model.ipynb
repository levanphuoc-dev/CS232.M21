{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHBP5xzmrM4U"
      },
      "source": [
        "# Neural Machine Translation (NMT) - Translating English sentences to Vietnam sentences\n",
        "\n",
        "Machine Translation refers to translating phrases across languages using deep learning and specifically with RNN ( Recurrent Neural Nets ). Most of these are complex systems that is they are a combined system of various algorithms. But, at its core, NMT uses sequence-to-sequence ( seq2seq ) RNN cells. Such models could be character level but word level models remain common.\n",
        "\n",
        "![NMT system](https://3.bp.blogspot.com/-3Pbj_dvt0Vo/V-qe-Nl6P5I/AAAAAAAABQc/z0_6WtVWtvARtMk0i9_AtLeyyGyV6AI4wCLcB/s1600/nmt-model-fast.gif)\n",
        "\n",
        "I insist to change the runtime to a GPU runtime so that training could be faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVPRC0bOtBas"
      },
      "source": [
        "## What are we going to do?\n",
        "We will basically create an encoder-decoder LSTM model using [Keras Functional API](https://www.tensorflow.org/alpha/guide/keras/functional) ( with [TensorFlow](https://www.tensorflow.org/) ). We will convert the English sentences to [Marathi](https://en.wikipedia.org/wiki/Marathi_language) ( A language native to India ). But, why Marathi?\n",
        "\n",
        "\n",
        "*   Has special characters and much complex.\n",
        "*   Has a totally new script ( Devnagiri ) with no pretrained word-embeddings available yet.\n",
        "\n",
        "Here's an example,\n",
        "\n",
        "Hello --> Xin chào\n",
        "\n",
        "So, let's get started.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_r70epHozOt"
      },
      "source": [
        "## Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq4aH4u1uq5V"
      },
      "source": [
        "### 1) Importing the libraries\n",
        "\n",
        "We will import TensorFlow and Keras. From Keras, we import various modules which help in building NN layers, preprocess data and construct LSTM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qK2TWV1nm48Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnIg8HdTGW4o"
      },
      "source": [
        "### 2) Reading the data\n",
        "\n",
        "\n",
        "Our dataset which contains more than 30K pairs of English-Marathi phrases. This amazing dataset is available at http://www.manythings.org/anki/ and it also other 50+ sets of bilingual sentences. We download the dataset for English-Marathi phrases, unzip it and read it using [Pandas](https://pandas.pydata.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27OzmS-MIymc",
        "outputId": "8cab011a-d3f9-41cf-f5d9-24c7e07196fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-20 03:00:57--  http://www.manythings.org/anki/vie-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.186.54, 104.21.92.44, 2606:4700:3030::6815:5c2c, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.186.54|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 320614 (313K) [application/zip]\n",
            "Saving to: ‘vie-eng.zip’\n",
            "\n",
            "vie-eng.zip         100%[===================>] 313.10K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-20 03:00:58 (2.17 MB/s) - ‘vie-eng.zip’ saved [320614/320614]\n",
            "\n",
            "Archive:  vie-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: vie.txt                 \n"
          ]
        }
      ],
      "source": [
        "\n",
        "!wget http://www.manythings.org/anki/vie-eng.zip -O vie-eng.zip\n",
        "!unzip vie-eng.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wKCgjP61nxxO"
      },
      "outputs": [],
      "source": [
        "lines = pd.read_table( 'vie.txt' , names=[ 'eng' , 'vie' ] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JsCTBHaOn6qu"
      },
      "outputs": [],
      "source": [
        "lines.reset_index( level=0 , inplace=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3hFTEqE7n74G",
        "outputId": "70159468-bf18-4a03-879d-e102ef986907"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>eng</th>\n",
              "      <th>vie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Chạy!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Help!</td>\n",
              "      <td>Giúp tôi với!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go on.</td>\n",
              "      <td>Tiếp tục đi.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>Chào bạn.</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hurry!</td>\n",
              "      <td>Nhanh lên nào!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8045</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Sê-lê-na Gô-mét đã được lựa chọn...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8046</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Selena Gomez đã được lựa chọn để...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8047</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Selena Gomez đã trở thành Đại sứ...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8048</th>\n",
              "      <td>The people here are particular about what they...</td>\n",
              "      <td>Những người ở đây khá là khó tính về khẩu vị ă...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8049</th>\n",
              "      <td>No matter how much you try to convince people ...</td>\n",
              "      <td>Cho dù bạn có thuyết phục mọi người rằng sô-cô...</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8050 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  index  \\\n",
              "0                                                  Run!   \n",
              "1                                                 Help!   \n",
              "2                                                Go on.   \n",
              "3                                                Hello!   \n",
              "4                                                Hurry!   \n",
              "...                                                 ...   \n",
              "8045  In 2009, Selena Gomez became the youngest pers...   \n",
              "8046  In 2009, Selena Gomez became the youngest pers...   \n",
              "8047  In 2009, Selena Gomez became the youngest pers...   \n",
              "8048  The people here are particular about what they...   \n",
              "8049  No matter how much you try to convince people ...   \n",
              "\n",
              "                                                    eng  \\\n",
              "0                                                 Chạy!   \n",
              "1                                         Giúp tôi với!   \n",
              "2                                          Tiếp tục đi.   \n",
              "3                                             Chào bạn.   \n",
              "4                                        Nhanh lên nào!   \n",
              "...                                                 ...   \n",
              "8045  Vào năm 2009, Sê-lê-na Gô-mét đã được lựa chọn...   \n",
              "8046  Vào năm 2009, Selena Gomez đã được lựa chọn để...   \n",
              "8047  Vào năm 2009, Selena Gomez đã trở thành Đại sứ...   \n",
              "8048  Những người ở đây khá là khó tính về khẩu vị ă...   \n",
              "8049  Cho dù bạn có thuyết phục mọi người rằng sô-cô...   \n",
              "\n",
              "                                                    vie  \n",
              "0     CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
              "1     CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
              "2     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "3     CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
              "4     CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
              "...                                                 ...  \n",
              "8045  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "8046  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "8047  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
              "8048  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
              "8049  CC-BY 2.0 (France) Attribution: tatoeba.org #9...  \n",
              "\n",
              "[8050 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g_ads9smoIOo"
      },
      "outputs": [],
      "source": [
        "lines.rename( columns={ 'index' : 'eng' , 'eng' : 'vie' , 'vie' : 'c' } , inplace=True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dgIdfjIRLDN"
      },
      "source": [
        "### 3) Preparing input data for the Encoder ( `encoder_input_data` )\n",
        "The Encoder model will be fed input data which are preprocessed English sentences. The preprocessing is done as follows :\n",
        "\n",
        "\n",
        "1.   Tokenizing the English sentences from `eng_lines`.\n",
        "2.   Determining the maximum length of the English sentence that's `max_input_length`.\n",
        "3.   Padding the `tokenized_eng_lines` to the max_input_length.\n",
        "4.   Determining the vocabulary size ( `num_eng_tokens` ) for English words.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xk5qXb6kpPxW"
      },
      "outputs": [],
      "source": [
        "eng_lines = list()\n",
        "for line in lines.eng:\n",
        "    eng_lines.append( line ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "up3Yfut1pYol"
      },
      "outputs": [],
      "source": [
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( eng_lines ) \n",
        "tokenized_eng_lines = tokenizer.texts_to_sequences( eng_lines ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McI0G_HnphMt",
        "outputId": "8484fb8e-8878-430b-c0fc-1ed431b37be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English max length is 32\n"
          ]
        }
      ],
      "source": [
        "length_list = list()\n",
        "for token_seq in tokenized_eng_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_input_length = np.array( length_list ).max()\n",
        "print( 'English max length is {}'.format( max_input_length ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nazp0FX5plVF",
        "outputId": "84a7a603-160d-4f86-e360-c29d2b421be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder input data shape -> (8050, 32)\n"
          ]
        }
      ],
      "source": [
        "padded_eng_lines = preprocessing.sequence.pad_sequences( tokenized_eng_lines , maxlen=max_input_length , padding='post' )\n",
        "encoder_input_data = np.array( padded_eng_lines )\n",
        "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2_ux1rZnDyY",
        "outputId": "a3d00d4c-30d3-477d-f67f-e554500203f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of English tokens = 3802\n"
          ]
        }
      ],
      "source": [
        "eng_word_dict = tokenizer.word_index\n",
        "num_eng_tokens = len( eng_word_dict )+1\n",
        "print( 'Number of English tokens = {}'.format( num_eng_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRwAd310SPkG"
      },
      "source": [
        "### 4) Preparing input data for the Decoder ( `decoder_input_data` )\n",
        "The Decoder model will be fed the preprocessed Marathi lines. The preprocessing steps are similar to the ones which are above. This one step is carried out before the other steps.\n",
        "\n",
        "\n",
        "*   Append `<START>` tag at the first position in  each Marathi sentence.\n",
        "*   Append `<END>` tag at the last position in  each Marathi sentence.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0a022jLIprId"
      },
      "outputs": [],
      "source": [
        "vie_lines = list()\n",
        "for line in lines.vie:\n",
        "    vie_lines.append( '<START> ' + line + ' <END>' )  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iNeYte1fqEte"
      },
      "outputs": [],
      "source": [
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( vie_lines ) \n",
        "tokenized_vie_lines = tokenizer.texts_to_sequences( vie_lines ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GulPMF4vqNlF",
        "outputId": "374527c1-bb26-494f-db24-c89d77839428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vietnam max length is 43\n"
          ]
        }
      ],
      "source": [
        "length_list = list()\n",
        "for token_seq in tokenized_vie_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list ).max()\n",
        "print( 'Vietnam max length is {}'.format( max_output_length ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ft5igLnqTAe",
        "outputId": "2e7fcdee-bdde-49dd-b4fc-35471acacadb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder input data shape -> (8050, 43)\n"
          ]
        }
      ],
      "source": [
        "padded_vie_lines = preprocessing.sequence.pad_sequences( tokenized_vie_lines , maxlen=max_output_length, padding='post' )\n",
        "decoder_input_data = np.array( padded_vie_lines )\n",
        "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deB0oX_0pj8R",
        "outputId": "cee8bafb-0bed-4634-85a1-4e8035aa4f35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Vietnam tokens = 2384\n"
          ]
        }
      ],
      "source": [
        "vie_word_dict = tokenizer.word_index\n",
        "num_vie_tokens = len( vie_word_dict )+1\n",
        "print( 'Number of Vietnam tokens = {}'.format( num_vie_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJTcSlygTQ_V"
      },
      "source": [
        "### 5) Preparing target data for the Decoder ( decoder_target_data ) \n",
        "\n",
        "We take a copy of `tokenized_mar_lines` and modify it like this.\n",
        "\n",
        "\n",
        "\n",
        "1.   We remove the `<start>` tag which we appended earlier. Hence, the word ( which is `<start>` in this case  ) will be removed.\n",
        "2.   Convert the `padded_mar_lines` ( ones which do not have `<start>` tag ) to one-hot vectors.\n",
        "\n",
        "For example :\n",
        "\n",
        "```\n",
        " [ '<start>' , 'hello' , 'world' , '<end>' ]\n",
        "\n",
        "```\n",
        "\n",
        "wil become \n",
        "\n",
        "```\n",
        " [ 'hello' , 'world' , '<end>' ]\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPCTmeL7qj3T",
        "outputId": "bfa122bd-a450-49df-b170-92696bf75d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder target data shape -> (8050, 43, 2384)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "decoder_target_data = list()\n",
        "for token_seq in tokenized_vie_lines:\n",
        "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
        "    \n",
        "padded_vie_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
        "onehot_vie_lines = utils.to_categorical( padded_vie_lines , num_vie_tokens )\n",
        "decoder_target_data = np.array( onehot_vie_lines )\n",
        "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KS5gWlcpFT1"
      },
      "source": [
        "## Defining and Training the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_N71uykUPbe"
      },
      "source": [
        "### 1) Defining the Encoder-Decoder model\n",
        "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
        "\n",
        "\n",
        "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
        "*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n",
        "*   LSTM layer : Provide access to Long-Short Term cells.\n",
        "\n",
        "Working : \n",
        "\n",
        "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
        "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
        "3.   These states are set in the LSTM cell of the decoder.\n",
        "4.   The decoder_input_data comes in through the Embedding layer.\n",
        "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqb4Bps1s_Lr",
        "outputId": "b83b72f0-8c62-4514-e1d4-8cacdfdd3415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Phuoc\\anaconda3\\envs\\tf-older\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From c:\\Users\\Phuoc\\anaconda3\\envs\\tf-older\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From c:\\Users\\Phuoc\\anaconda3\\envs\\tf-older\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 256)    973312      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    610304      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 128), (None, 197120      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 128),  197120      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 2384)   307536      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,285,392\n",
            "Trainable params: 2,285,392\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( num_eng_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 128 , return_state=True  )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( num_vie_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 128 , return_state=True , return_sequences=True)\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( num_vie_tokens , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9g_8sR7WWf3"
      },
      "source": [
        "### 2) Training the model\n",
        "We train the model for a number of epochs with RMSprop optimizer and categorical crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 8050 samples\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.42992, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 84s - loss: 1.4299\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: loss improved from 1.42992 to 1.24880, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 93s - loss: 1.2488\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: loss improved from 1.24880 to 1.20099, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 94s - loss: 1.2010\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: loss improved from 1.20099 to 1.16324, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 102s - loss: 1.1632\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: loss improved from 1.16324 to 1.13065, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 93s - loss: 1.1307\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: loss improved from 1.13065 to 1.10049, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 94s - loss: 1.1005\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: loss improved from 1.10049 to 1.07204, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 96s - loss: 1.0720\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: loss improved from 1.07204 to 1.04373, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 111s - loss: 1.0437\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: loss improved from 1.04373 to 1.01711, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 98s - loss: 1.0171\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: loss improved from 1.01711 to 0.99204, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 91s - loss: 0.9920\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: loss improved from 0.99204 to 0.96827, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 92s - loss: 0.9683\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: loss improved from 0.96827 to 0.94650, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 93s - loss: 0.9465\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: loss improved from 0.94650 to 0.92612, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 94s - loss: 0.9261\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: loss improved from 0.92612 to 0.90760, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 94s - loss: 0.9076\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: loss improved from 0.90760 to 0.88860, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 93s - loss: 0.8886\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: loss improved from 0.88860 to 0.87109, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 94s - loss: 0.8711\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: loss improved from 0.87109 to 0.85349, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 91s - loss: 0.8535\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: loss improved from 0.85349 to 0.83629, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.8363\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: loss improved from 0.83629 to 0.81995, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 93s - loss: 0.8200\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: loss improved from 0.81995 to 0.80355, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 96s - loss: 0.8035\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: loss improved from 0.80355 to 0.78747, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.7875\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: loss improved from 0.78747 to 0.77140, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 91s - loss: 0.7714\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: loss improved from 0.77140 to 0.75619, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 93s - loss: 0.7562\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: loss improved from 0.75619 to 0.74131, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 94s - loss: 0.7413\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: loss improved from 0.74131 to 0.72657, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 95s - loss: 0.7266\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: loss improved from 0.72657 to 0.71189, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 92s - loss: 0.7119\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: loss improved from 0.71189 to 0.69800, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 92s - loss: 0.6980\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: loss improved from 0.69800 to 0.68430, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 95s - loss: 0.6843\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: loss improved from 0.68430 to 0.66997, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 107s - loss: 0.6700\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: loss improved from 0.66997 to 0.65674, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 101s - loss: 0.6567\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: loss improved from 0.65674 to 0.64378, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 95s - loss: 0.6438\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: loss improved from 0.64378 to 0.63080, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 93s - loss: 0.6308\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: loss improved from 0.63080 to 0.61725, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 98s - loss: 0.6173\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: loss improved from 0.61725 to 0.60566, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 107s - loss: 0.6057\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: loss improved from 0.60566 to 0.59286, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 103s - loss: 0.5929\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: loss improved from 0.59286 to 0.58047, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 100s - loss: 0.5805\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: loss improved from 0.58047 to 0.56892, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 100s - loss: 0.5689\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: loss improved from 0.56892 to 0.55719, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 101s - loss: 0.5572\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: loss improved from 0.55719 to 0.54491, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 104s - loss: 0.5449\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: loss improved from 0.54491 to 0.53394, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 101s - loss: 0.5339\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: loss improved from 0.53394 to 0.52219, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 100s - loss: 0.5222\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: loss improved from 0.52219 to 0.51182, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 99s - loss: 0.5118\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: loss improved from 0.51182 to 0.50126, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 95s - loss: 0.5013\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: loss improved from 0.50126 to 0.49038, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 105s - loss: 0.4904\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: loss improved from 0.49038 to 0.47984, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 99s - loss: 0.4798\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: loss improved from 0.47984 to 0.46929, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 95s - loss: 0.4693\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: loss improved from 0.46929 to 0.45917, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 101s - loss: 0.4592\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: loss improved from 0.45917 to 0.44983, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 103s - loss: 0.4498\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: loss improved from 0.44983 to 0.44021, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 103s - loss: 0.4402\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: loss improved from 0.44021 to 0.43083, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 105s - loss: 0.4308\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: loss improved from 0.43083 to 0.42065, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 101s - loss: 0.4206\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: loss improved from 0.42065 to 0.41199, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 93s - loss: 0.4120\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: loss improved from 0.41199 to 0.40305, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 92s - loss: 0.4031\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: loss improved from 0.40305 to 0.39408, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 91s - loss: 0.3941\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: loss improved from 0.39408 to 0.38510, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 91s - loss: 0.3851\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: loss improved from 0.38510 to 0.37735, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 91s - loss: 0.3774\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: loss improved from 0.37735 to 0.36875, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 91s - loss: 0.3688\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: loss improved from 0.36875 to 0.36030, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.3603\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: loss improved from 0.36030 to 0.35258, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.3526\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: loss improved from 0.35258 to 0.34514, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.3451\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: loss improved from 0.34514 to 0.33662, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.3366\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: loss improved from 0.33662 to 0.33001, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 91s - loss: 0.3300\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: loss improved from 0.33001 to 0.32180, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 92s - loss: 0.3218\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: loss improved from 0.32180 to 0.31485, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.3149\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: loss improved from 0.31485 to 0.30734, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.3073\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: loss improved from 0.30734 to 0.30073, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.3007\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: loss improved from 0.30073 to 0.29463, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 91s - loss: 0.2946\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: loss improved from 0.29463 to 0.28710, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.2871\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: loss improved from 0.28710 to 0.28119, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.2812\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: loss improved from 0.28119 to 0.27405, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 92s - loss: 0.2740\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: loss improved from 0.27405 to 0.26879, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 89s - loss: 0.2688\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: loss improved from 0.26879 to 0.26165, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.2617\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: loss improved from 0.26165 to 0.25620, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.2562\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: loss improved from 0.25620 to 0.24980, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 89s - loss: 0.2498\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: loss improved from 0.24980 to 0.24406, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 89s - loss: 0.2441\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: loss improved from 0.24406 to 0.23907, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 89s - loss: 0.2391\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: loss improved from 0.23907 to 0.23251, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.2325\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: loss improved from 0.23251 to 0.22723, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.2272\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: loss improved from 0.22723 to 0.22253, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.2225\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: loss improved from 0.22253 to 0.21747, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.2175\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: loss improved from 0.21747 to 0.21229, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.2123\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: loss improved from 0.21229 to 0.20698, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 89s - loss: 0.2070\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: loss improved from 0.20698 to 0.20223, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 92s - loss: 0.2022\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: loss improved from 0.20223 to 0.19826, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 90s - loss: 0.1983\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: loss improved from 0.19826 to 0.19256, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 98s - loss: 0.1926\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: loss improved from 0.19256 to 0.18885, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 100s - loss: 0.1889\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: loss improved from 0.18885 to 0.18437, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 109s - loss: 0.1844\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: loss improved from 0.18437 to 0.17940, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 102s - loss: 0.1794\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: loss improved from 0.17940 to 0.17565, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 120s - loss: 0.1756\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: loss improved from 0.17565 to 0.17225, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 136s - loss: 0.1722\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: loss improved from 0.17225 to 0.16700, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 129s - loss: 0.1670\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: loss improved from 0.16700 to 0.16390, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 125s - loss: 0.1639\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: loss improved from 0.16390 to 0.15946, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 127s - loss: 0.1595\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: loss improved from 0.15946 to 0.15552, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 126s - loss: 0.1555\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: loss improved from 0.15552 to 0.15251, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 124s - loss: 0.1525\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: loss improved from 0.15251 to 0.14844, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 125s - loss: 0.1484\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: loss improved from 0.14844 to 0.14452, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 123s - loss: 0.1445\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: loss improved from 0.14452 to 0.14149, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 123s - loss: 0.1415\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: loss improved from 0.14149 to 0.13842, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 123s - loss: 0.1384\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: loss improved from 0.13842 to 0.13514, saving model to saveModel/model_eng_vie_4.h5\n",
            "8050/8050 - 123s - loss: 0.1351\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x22c525a8c50>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filename = 'saveModel/model_eng_vie_4.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=100, callbacks=[checkpoint], verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnd2H27qt4Hy",
        "outputId": "8a187e01-8a46-446e-f6c1-9544d1672c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 8050 samples\n",
            "Epoch 1/20\n",
            "8050/8050 [==============================] - 91s 11ms/sample - loss: 1.4289\n",
            "Epoch 2/20\n",
            "8050/8050 [==============================] - 113s 14ms/sample - loss: 1.2497\n",
            "Epoch 3/20\n",
            "8050/8050 [==============================] - 138s 17ms/sample - loss: 1.2070\n",
            "Epoch 4/20\n",
            "8050/8050 [==============================] - 131s 16ms/sample - loss: 1.1714\n",
            "Epoch 5/20\n",
            "8050/8050 [==============================] - 131s 16ms/sample - loss: 1.1415\n",
            "Epoch 6/20\n",
            "8050/8050 [==============================] - 143s 18ms/sample - loss: 1.1130\n",
            "Epoch 7/20\n",
            "8050/8050 [==============================] - 145s 18ms/sample - loss: 1.0832\n",
            "Epoch 8/20\n",
            "8050/8050 [==============================] - 148s 18ms/sample - loss: 1.0517\n",
            "Epoch 9/20\n",
            "8050/8050 [==============================] - 142s 18ms/sample - loss: 1.0212\n",
            "Epoch 10/20\n",
            "8050/8050 [==============================] - 140s 17ms/sample - loss: 0.9943\n",
            "Epoch 11/20\n",
            "8050/8050 [==============================] - 155s 19ms/sample - loss: 0.9705\n",
            "Epoch 12/20\n",
            "8050/8050 [==============================] - 150s 19ms/sample - loss: 0.9482\n",
            "Epoch 13/20\n",
            "8050/8050 [==============================] - 156s 19ms/sample - loss: 0.9283\n",
            "Epoch 14/20\n",
            "8050/8050 [==============================] - 155s 19ms/sample - loss: 0.9097\n",
            "Epoch 15/20\n",
            "8050/8050 [==============================] - 149s 18ms/sample - loss: 0.8911\n",
            "Epoch 16/20\n",
            "8050/8050 [==============================] - 162s 20ms/sample - loss: 0.8735\n",
            "Epoch 17/20\n",
            "8050/8050 [==============================] - 146s 18ms/sample - loss: 0.8561\n",
            "Epoch 18/20\n",
            "8050/8050 [==============================] - 138s 17ms/sample - loss: 0.8393\n",
            "Epoch 19/20\n",
            "8050/8050 [==============================] - 152s 19ms/sample - loss: 0.8226\n",
            "Epoch 20/20\n",
            "8050/8050 [==============================] - 193s 24ms/sample - loss: 0.8069\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x221b58d9e80>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=20) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "i0BzAIoyrwwl"
      },
      "outputs": [],
      "source": [
        "# model.save( 'saveModel/model_eng_vie.hdf5' ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r6ZyPVIRPXII"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Phuoc\\anaconda3\\envs\\tf-older\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From c:\\Users\\Phuoc\\anaconda3\\envs\\tf-older\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From c:\\Users\\Phuoc\\anaconda3\\envs\\tf-older\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('saveModel/model_eng_vie_3.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eeqv_vH5pMpb"
      },
      "source": [
        "## Inferencing on the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4PAtzGrk8pq"
      },
      "source": [
        "### 1) Defining inference models\n",
        "We create inference models which help in predicting translations.\n",
        "\n",
        "**Encoder inference model** : Takes the English sentence as input and outputs LSTM states ( `h` and `c` ).\n",
        "\n",
        "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the Marathi input seqeunces ( ones not having the `<start>` tag ). It will output the translations of the English sentence which we fed to the encoder model and its state values.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UNhVkiZLvdTq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 128 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 128 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djEPrfJBmZE-"
      },
      "source": [
        "### 2) Making some translations\n",
        "\n",
        "\n",
        "1.   First, we take a English sequence and predict the state values using `enc_model`.\n",
        "2.   We set the state values in the decoder's LSTM.\n",
        "3.   Then, we generate a sequence which contains the `<start>` element.\n",
        "4.   We input this sequence in the `dec_model`.\n",
        "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
        "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum sequence length.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Y_hrJcNP-mXb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( eng_word_dict[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unknown error occured\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-22-be6f39c1e8c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m#listens for the user's input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0maudio2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# Using google to recognize audio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Phuoc\\anaconda3\\envs\\tf-older\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mlisten\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    650\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m                 \u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCHUNK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m  \u001b[1;31m# reached end of the stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m                 \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Phuoc\\anaconda3\\envs\\tf-older\\lib\\site-packages\\speech_recognition\\__init__.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyaudio_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Phuoc\\anaconda3\\envs\\tf-older\\lib\\site-packages\\pyaudio.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    606\u001b[0m                           paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception_on_overflow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_read_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import playsound\n",
        "import os\n",
        "\n",
        "# Initialize the recognizer\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Loop infinitely for user to\n",
        "# speak\n",
        " \n",
        "while(1):   \n",
        "     \n",
        "    # Exception handling to handle\n",
        "    # exceptions at the runtime\n",
        "    try:\n",
        "         \n",
        "        # use the microphone as source for input.\n",
        "        with sr.Microphone() as source2:\n",
        "             \n",
        "            # wait for a second to let the recognizer\n",
        "            # adjust the energy threshold based on\n",
        "            # the surrounding noise level\n",
        "            r.adjust_for_ambient_noise(source2, duration=0.2)\n",
        "             \n",
        "            #listens for the user's input\n",
        "            audio2 = r.listen(source2)\n",
        "             \n",
        "            # Using google to recognize audio\n",
        "            MyText = r.recognize_google(audio2)\n",
        "            MyText = MyText.lower()\n",
        " \n",
        "            print(\"Did you say \"+MyText)\n",
        "\n",
        "            enc_model , dec_model = make_inference_models()\n",
        "\n",
        "            states_values = enc_model.predict( str_to_tokens( MyText ) )\n",
        "            empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "            empty_target_seq[0, 0] = vie_word_dict['start']\n",
        "            stop_condition = False\n",
        "            decoded_translation = ''\n",
        "            while not stop_condition :\n",
        "                dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "                sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "                sampled_word = None\n",
        "                for word , index in vie_word_dict.items() :\n",
        "                    if sampled_word_index == index :\n",
        "                        decoded_translation += ' {}'.format( word )\n",
        "                        sampled_word = word\n",
        "                \n",
        "                if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "                    stop_condition = True\n",
        "                    \n",
        "                empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "                empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "                states_values = [ h , c ] \n",
        "\n",
        "            print( decoded_translation )    \n",
        "            \n",
        "            # text = \"Em nhà ở đâu thế\" \n",
        "            output = gTTS(decoded_translation, lang=\"vi\", slow=False)\n",
        "            output.save(\"output.mp3\")\n",
        "            playsound.playsound('output.mp3', True)\n",
        "            os.remove(\"output.mp3\")\n",
        "             \n",
        "    except sr.RequestError as e:\n",
        "        print(\"Could not request results; {0}\".format(e))\n",
        "         \n",
        "    except sr.UnknownValueError:\n",
        "        print(\"unknown error occured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " nhớ điểm thống thút gratin với súng cực cực cực cực ủng cực cực đời đời to ủng đội cãi lành lành sắp sắp sắp đợt cocktail làn chút nhạc cuả dâu chay nĩa phố góc waikiki khí nghĩa trai lồ dẹp thứ tàng\n"
          ]
        }
      ],
      "source": [
        "\n",
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "states_values = enc_model.predict( str_to_tokens( 'hello' ) )\n",
        "empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "empty_target_seq[0, 0] = vie_word_dict['start']\n",
        "stop_condition = False\n",
        "decoded_translation = ''\n",
        "while not stop_condition :\n",
        "    dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "    sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "    sampled_word = None\n",
        "    for word , index in vie_word_dict.items() :\n",
        "        if sampled_word_index == index :\n",
        "            decoded_translation += ' {}'.format( word )\n",
        "            sampled_word = word\n",
        "    \n",
        "    if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "        stop_condition = True\n",
        "        \n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "    empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "    states_values = [ h , c ] \n",
        "\n",
        "print( decoded_translation )\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "id": "2Mfco9WKukhS",
        "outputId": "e2383ceb-cd50-4d89-9a7f-1d18ad8f33bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter eng sentence : hello\n",
            " chào bạn end\n",
            "Enter eng sentence : how are you\n",
            " bạn thế nào end\n",
            "Enter eng sentence : help me\n",
            " cứu tôi với end\n",
            "Enter eng sentence : come here\n",
            " lại đây nào end\n",
            "Enter eng sentence : are you ok\n",
            " bạn có sao không end\n",
            "Enter eng sentence : how old are you\n",
            " bạn bao nhiêu tuổi end\n",
            "Enter eng sentence : please get me hotel security\n",
            " làm ơn cho tôi gặp bảo vệ khách sạn end\n",
            "Enter eng sentence : my hobby is taking pictures of wild flowers\n",
            " sở thích của tôi là chụp những bức ảnh hoa dại end\n",
            "Enter eng sentence : people used to think that only humans could use language\n",
            " mọi người thường nghĩ rằng chỉ có con người mới có thể sử dụng ngôn nghĩ end\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-2f943ab01d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter eng sentence : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#states_values = enc_model.predict( encoder_input_data[ epoch ] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# enc_model , dec_model = make_inference_models()\n",
        "\n",
        "# for epoch in range( encoder_input_data.shape[0] ):\n",
        "#     states_values = enc_model.predict( str_to_tokens( input( 'Enter eng sentence : ' ) ) )\n",
        "#     #states_values = enc_model.predict( encoder_input_data[ epoch ] )\n",
        "#     empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "#     empty_target_seq[0, 0] = vie_word_dict['start']\n",
        "#     stop_condition = False\n",
        "#     decoded_translation = ''\n",
        "#     while not stop_condition :\n",
        "#         dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "#         sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "#         sampled_word = None\n",
        "#         for word , index in vie_word_dict.items() :\n",
        "#             if sampled_word_index == index :\n",
        "#                 decoded_translation += ' {}'.format( word )\n",
        "#                 sampled_word = word\n",
        "        \n",
        "#         if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "#             stop_condition = True\n",
        "            \n",
        "#         empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "#         empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "#         states_values = [ h , c ] \n",
        "\n",
        "#     print( decoded_translation )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": " BT7_19522054_LeVanPhuoc.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.13 ('tf-older')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "56e30a9158a0cc829a434d6cba64b19ee75ea7478f27ef0e4c89775a79ded9fe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
